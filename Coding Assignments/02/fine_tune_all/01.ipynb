{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: evaluate in c:\\users\\hp\\anaconda3\\lib\\site-packages (0.4.0)\n",
      "Requirement already satisfied: datasets>=2.0.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from evaluate) (2.13.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from evaluate) (1.21.5)\n",
      "Requirement already satisfied: dill in c:\\users\\hp\\anaconda3\\lib\\site-packages (from evaluate) (0.3.6)\n",
      "Requirement already satisfied: pandas in c:\\users\\hp\\anaconda3\\lib\\site-packages (from evaluate) (1.4.4)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from evaluate) (2.28.1)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from evaluate) (4.64.1)\n",
      "Requirement already satisfied: xxhash in c:\\users\\hp\\anaconda3\\lib\\site-packages (from evaluate) (3.2.0)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\hp\\anaconda3\\lib\\site-packages (from evaluate) (0.70.14)\n",
      "Requirement already satisfied: fsspec[http]>=2021.05.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from evaluate) (2022.7.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from evaluate) (0.16.4)\n",
      "Requirement already satisfied: packaging in c:\\users\\hp\\anaconda3\\lib\\site-packages (from evaluate) (21.3)\n",
      "Requirement already satisfied: responses<0.19 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from evaluate) (0.18.0)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from datasets>=2.0.0->evaluate) (12.0.1)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\hp\\anaconda3\\lib\\site-packages (from datasets>=2.0.0->evaluate) (3.8.4)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from datasets>=2.0.0->evaluate) (6.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\hp\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.7.0->evaluate) (3.12.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.7.0->evaluate) (4.3.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from packaging->evaluate) (3.0.9)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests>=2.19.0->evaluate) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests>=2.19.0->evaluate) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests>=2.19.0->evaluate) (1.26.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests>=2.19.0->evaluate) (2022.9.14)\n",
      "Requirement already satisfied: colorama in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tqdm>=4.62.1->evaluate) (0.4.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from pandas->evaluate) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from pandas->evaluate) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.1->pandas->evaluate) (1.16.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (21.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.8.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2 -> 23.2.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: accelerate in c:\\users\\hp\\anaconda3\\lib\\site-packages (0.22.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from accelerate) (1.21.5)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from accelerate) (21.3)\n",
      "Requirement already satisfied: psutil in c:\\users\\hp\\anaconda3\\lib\\site-packages (from accelerate) (5.9.0)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\hp\\anaconda3\\lib\\site-packages (from accelerate) (6.0)\n",
      "Requirement already satisfied: torch>=1.10.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from accelerate) (1.13.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from packaging>=20.0->accelerate) (3.0.9)\n",
      "Requirement already satisfied: typing_extensions in c:\\users\\hp\\anaconda3\\lib\\site-packages (from torch>=1.10.0->accelerate) (4.3.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2 -> 23.2.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\hp\\anaconda3\\lib\\site-packages (4.33.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\hp\\anaconda3\\lib\\site-packages (from transformers) (3.12.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from transformers) (0.16.4)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from transformers) (1.21.5)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from transformers) (2022.7.9)\n",
      "Requirement already satisfied: requests in c:\\users\\hp\\anaconda3\\lib\\site-packages (from transformers) (2.28.1)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from transformers) (0.3.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from transformers) (4.64.1)\n",
      "Requirement already satisfied: fsspec in c:\\users\\hp\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (2022.7.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (4.3.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from packaging>=20.0->transformers) (3.0.9)\n",
      "Requirement already satisfied: colorama in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.5)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests->transformers) (1.26.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests->transformers) (2022.9.14)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2 -> 23.2.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install evaluate\n",
    "! pip install -U accelerate\n",
    "! pip install -U transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import requests\n",
    "import torch\n",
    "\n",
    "from datasets import load_dataset, load_from_disk, Dataset, Features, Array3D\n",
    "from io import BytesIO\n",
    "from transformers import AutoProcessor, ViTFeatureExtractor, ViTForImageClassification, Trainer, TrainingArguments, default_data_collator\n",
    "from typing import Tuple\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The directory where our images are saved in folders by category\n",
    "images_dir = \"2750\"\n",
    "\n",
    "# The output directory of the processed datasets\n",
    "train_save_path = \"./processed-datasets/train\"\n",
    "val_save_path = \"./processed-datasets/val\"\n",
    "test_save_path = \"./processed-datasets/test\"\n",
    "\n",
    "\n",
    "# Sizes of dataset splits\n",
    "val_size = 0.2\n",
    "test_size = 0.1\n",
    "\n",
    "# Name of model as named in the HuggingFace Hub\n",
    "model_name = \"google/vit-base-patch16-224-in21k\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23af0b2383134ebbb3a6be9729242147",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset imagefolder/default to C:/Users/hp/.cache/huggingface/datasets/imagefolder/default-4fcfc1c3340bae97/0.0.0/37fbb85cc714a338bea574ac6c7d0b5be5aff46c1862c1989b20e0771199e93f...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "677b865e1ab74472be6c51ae08f4afa6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e32a934ca9594643882aa439f8122e61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27fcb417e11640b1b1f1e4b97b8050c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0c7886d49dd4ed8b2ffe54274e39d55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset imagefolder downloaded and prepared to C:/Users/hp/.cache/huggingface/datasets/imagefolder/default-4fcfc1c3340bae97/0.0.0/37fbb85cc714a338bea574ac6c7d0b5be5aff46c1862c1989b20e0771199e93f. Subsequent calls will reuse this data.\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"imagefolder\", data_dir=images_dir, split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting dataset into train, validation and test sets...\n"
     ]
    }
   ],
   "source": [
    "def split_dataset(\n",
    "    dataset: Dataset,\n",
    "    val_size: float=0.2,\n",
    "    test_size: float=0.1\n",
    ") -> Tuple[Dataset, Dataset, Dataset]:\n",
    "    \"\"\"\n",
    "    Returns a tuple with three random train, validation and test subsets by splitting the passed dataset.\n",
    "    Size of the validation and test sets defined as a fraction of 1 with the `val_size` and `test_size` arguments.\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Splitting dataset into train, validation and test sets...\")\n",
    "\n",
    "    # Split dataset into train and (val + test) sets\n",
    "    split_size = round(val_size + test_size, 3)\n",
    "    dataset = dataset.train_test_split(shuffle=True, test_size=split_size)\n",
    "\n",
    "    # Split (val + test) into val and test sets\n",
    "    split_ratio = round(test_size / (test_size + val_size), 3)\n",
    "    val_test_sets = dataset['test'].train_test_split(shuffle=True, test_size=split_ratio)\n",
    "\n",
    "    train_dataset = dataset[\"train\"]\n",
    "    val_dataset = val_test_sets[\"train\"]\n",
    "    test_dataset = val_test_sets[\"test\"]\n",
    "    return train_dataset, val_dataset, test_dataset\n",
    "\n",
    "\n",
    "# Split dataset into train and test sets\n",
    "train_dataset, val_dataset, test_dataset = split_dataset(dataset, val_size, test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eadc522b96e94db58d510c01ee5de467",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1400 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08674c04e94242938fbd97ad191d5d15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/400 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "618579fefcc04546ada1c5a182f46088",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def process_examples(examples, image_processor):\n",
    "    \"\"\"Processor helper function. Used to process batches of images using the\n",
    "    passed image_processor.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    examples\n",
    "        A batch of image examples.\n",
    "\n",
    "    image_processor\n",
    "        A HuggingFace image processor for the selected model.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    examples \n",
    "        A batch of processed image examples.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get batch of images\n",
    "    images = examples['image']\n",
    "\n",
    "    # Preprocess\n",
    "    inputs = image_processor(images=images)\n",
    "    # Add pixel_values\n",
    "    examples['pixel_values'] = inputs['pixel_values']\n",
    "\n",
    "    return examples\n",
    "\n",
    "\n",
    "def apply_processing(\n",
    "    model_name: str,\n",
    "    train_dataset: Dataset,\n",
    "    val_dataset: Dataset,\n",
    "    test_dataset: Dataset\n",
    ") -> Tuple[Dataset, Dataset, Dataset]:\n",
    "    \"\"\"\n",
    "    Apply model's image AutoProcessor to transform train, validation and test subsets.\n",
    "    Returns train, validation and test datasets with `pixel_values` in torch tensor type.\n",
    "    \"\"\"\n",
    "\n",
    "    # Extend the features \n",
    "    features = Features({\n",
    "        **train_dataset.features,\n",
    "        'pixel_values': Array3D(dtype=\"float32\", shape=(3, 224, 224)),\n",
    "    })\n",
    "\n",
    "    # Instantiate image_processor\n",
    "    image_processor = AutoProcessor.from_pretrained(model_name)\n",
    "    \n",
    "    # Preprocess images\n",
    "    train_dataset = train_dataset.map(process_examples, batched=True, features=features, fn_kwargs={\"image_processor\": image_processor})\n",
    "    val_dataset = val_dataset.map(process_examples, batched=True, features=features, fn_kwargs={\"image_processor\": image_processor})\n",
    "    test_dataset = test_dataset.map(process_examples, batched=True, features=features, fn_kwargs={\"image_processor\": image_processor})\n",
    "\n",
    "    # Set to torch format for training\n",
    "    train_dataset.set_format('torch', columns=['pixel_values', 'label'])\n",
    "    val_dataset.set_format('torch', columns=['pixel_values', 'label'])\n",
    "    test_dataset.set_format('torch', columns=['pixel_values', 'label'])\n",
    "    \n",
    "    # Remove unused column\n",
    "    train_dataset = train_dataset.remove_columns(\"image\")\n",
    "    val_dataset = val_dataset.remove_columns(\"image\")\n",
    "    test_dataset = test_dataset.remove_columns(\"image\")\n",
    "    \n",
    "    return train_dataset, val_dataset, test_dataset\n",
    "\n",
    "\n",
    "# Apply AutoProcessor\n",
    "train_dataset, val_dataset, test_dataset = apply_processing(model_name, \n",
    "train_dataset, val_dataset, test_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c483a37dd7ba4cd28a308da401d8cf6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1400 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55a342bf67eb4bf59bb654900c2d1c4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/400 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "489f2af59c5c4925b64eaa0e24534c67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Save train, validation and test preprocessed datasets\n",
    "train_dataset.save_to_disk(train_save_path, num_shards=1)\n",
    "\n",
    "val_dataset.save_to_disk(val_save_path, num_shards=1)\n",
    "\n",
    "test_dataset.save_to_disk(test_save_path, num_shards=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = load_from_disk(train_save_path)\n",
    "\n",
    "val_dataset = load_from_disk(val_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = train_dataset.features[\"label\"].num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\hp\\anaconda3\\lib\\site-packages\\transformers\\models\\vit\\feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Download model from model hub\n",
    "model = ViTForImageClassification.from_pretrained(model_name, num_labels=num_classes)\n",
    "\n",
    "# Download feature extractor from hub\n",
    "feature_extractor = ViTFeatureExtractor.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K for top accuracy metric\n",
    "k_for_top_acc = 3\n",
    "\n",
    "# Compute metrics function for binary classification\n",
    "acc_metric = evaluate.load(\"accuracy\", module_type=\"metric\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predicted_probs, labels = eval_pred\n",
    "    # Accuracy\n",
    "    predicted_labels = np.argmax(predicted_probs, axis=1)\n",
    "    acc = acc_metric.compute(predictions=predicted_labels, references=labels)\n",
    "    # Top-K Accuracy\n",
    "    top_k_indexes = [np.argpartition(row, -k_for_top_acc)[-k_for_top_acc:] for row in predicted_probs]\n",
    "    top_k_classes = [top_k_indexes[i][np.argsort(row[top_k_indexes[i]])] for i, row in enumerate(predicted_probs)]\n",
    "    top_k_classes = np.flip(np.array(top_k_classes), 1)\n",
    "    acc_k = {\n",
    "        f\"accuracy_k\" : sum([label in predictions for predictions, label in zip(top_k_classes, labels)]) / len(labels)\n",
    "    }\n",
    "    # Merge metrics\n",
    "    acc.update(acc_k)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change labels\n",
    "id2label = {key:train_dataset.features[\"label\"].names[index] for index,key in enumerate(model.config.id2label.keys())}\n",
    "label2id = {train_dataset.features[\"label\"].names[index]:value for index,value in enumerate(model.config.label2id.values())}\n",
    "model.config.id2label = id2label\n",
    "model.config.label2id = label2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = \"./model\"\n",
    "output_data_dir = \"./outputs\"\n",
    "\n",
    "# Total number of training epochs to perform\n",
    "num_train_epochs = 1\n",
    "# The batch size per GPU/TPU core/CPU for training\n",
    "per_device_train_batch_size = 32\n",
    "# The batch size per GPU/TPU core/CPU for evaluation\n",
    "per_device_eval_batch_size = 64\n",
    "# The initial learning rate for AdamW optimizer\n",
    "learning_rate = 2e-5\n",
    "# Number of steps used for a linear warmup from 0 to learning_rate\n",
    "warmup_steps = 500\n",
    "# The weight decay to apply to all layers except all bias and LayerNorm weights in AdamW optimizer\n",
    "weight_decay = 0.01\n",
    "\n",
    "main_metric_for_evaluation = \"accuracy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('4.33.1', '0.22.0')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import accelerate\n",
    "import transformers\n",
    "\n",
    "transformers.__version__, accelerate.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training args\n",
    "training_args = TrainingArguments(\n",
    "    output_dir = model_dir,\n",
    "    num_train_epochs = num_train_epochs,\n",
    "    per_device_train_batch_size = per_device_train_batch_size,\n",
    "    per_device_eval_batch_size = per_device_eval_batch_size,\n",
    "    warmup_steps = warmup_steps,\n",
    "    weight_decay = weight_decay,\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    save_strategy = \"epoch\",\n",
    "    logging_strategy = \"epoch\",\n",
    "    logging_dir = f\"{output_data_dir}/logs\",\n",
    "    learning_rate = float(learning_rate),\n",
    "    load_best_model_at_end = True,\n",
    "    metric_for_best_model = main_metric_for_evaluation,\n",
    ")\n",
    "\n",
    "# Create Trainer instance\n",
    "trainer = Trainer(\n",
    "    model = model,\n",
    "    args = training_args,\n",
    "    compute_metrics = compute_metrics,\n",
    "    train_dataset = train_dataset,\n",
    "    eval_dataset = val_dataset,\n",
    "    data_collator = default_data_collator,\n",
    "    tokenizer = feature_extractor\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9782978374341fa971b9df4acb361d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/44 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.3006, 'learning_rate': 1.76e-06, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09e04471b0ea41c1aa7eb283a4009df2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.286520004272461, 'eval_accuracy': 0.1175, 'eval_accuracy_k': 0.405, 'eval_runtime': 177.0542, 'eval_samples_per_second': 2.259, 'eval_steps_per_second': 0.04, 'epoch': 1.0}\n",
      "{'train_runtime': 2751.0231, 'train_samples_per_second': 0.509, 'train_steps_per_second': 0.016, 'train_loss': 2.3006241538307886, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=44, training_loss=2.3006241538307886, metrics={'train_runtime': 2751.0231, 'train_samples_per_second': 0.509, 'train_steps_per_second': 0.016, 'train_loss': 2.3006241538307886, 'epoch': 1.0})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>step</th>\n",
       "      <th>eval_loss</th>\n",
       "      <th>eval_accuracy</th>\n",
       "      <th>eval_accuracy_k</th>\n",
       "      <th>eval_runtime</th>\n",
       "      <th>eval_samples_per_second</th>\n",
       "      <th>eval_steps_per_second</th>\n",
       "      <th>train_runtime</th>\n",
       "      <th>train_samples_per_second</th>\n",
       "      <th>train_steps_per_second</th>\n",
       "      <th>total_flos</th>\n",
       "      <th>train_loss</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>epoch</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>2.3006</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>132</td>\n",
       "      <td>2.28652</td>\n",
       "      <td>0.1175</td>\n",
       "      <td>0.405</td>\n",
       "      <td>177.0542</td>\n",
       "      <td>2.259</td>\n",
       "      <td>0.04</td>\n",
       "      <td>2751.0231</td>\n",
       "      <td>0.509</td>\n",
       "      <td>0.016</td>\n",
       "      <td>1.084966e+17</td>\n",
       "      <td>2.300624</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         loss  learning_rate  step  eval_loss  eval_accuracy  eval_accuracy_k  \\\n",
       "epoch                                                                           \n",
       "1.0    2.3006       0.000002   132    2.28652         0.1175            0.405   \n",
       "\n",
       "       eval_runtime  eval_samples_per_second  eval_steps_per_second  \\\n",
       "epoch                                                                 \n",
       "1.0        177.0542                    2.259                   0.04   \n",
       "\n",
       "       train_runtime  train_samples_per_second  train_steps_per_second  \\\n",
       "epoch                                                                    \n",
       "1.0        2751.0231                     0.509                   0.016   \n",
       "\n",
       "         total_flos  train_loss  \n",
       "epoch                            \n",
       "1.0    1.084966e+17    2.300624  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "  log_history = pd.DataFrame(trainer.state.log_history)\n",
    "log_history = log_history.fillna(0)\n",
    "log_history = log_history.groupby(['epoch']).sum()\n",
    "log_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hp\\anaconda3\\lib\\site-packages\\transformers\\models\\vit\\feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f30df4b7e3e4d6ab7864dd4651c7567",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging evaluation results at ./outputs/eval_results.json\n",
      "{\n",
      "    \"eval_loss\": 2.2785730361938477,\n",
      "    \"eval_accuracy\": 0.14,\n",
      "    \"eval_accuracy_k\": 0.435,\n",
      "    \"eval_runtime\": 101.7511,\n",
      "    \"eval_samples_per_second\": 1.966,\n",
      "    \"eval_steps_per_second\": 0.246\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "test_dataset = load_from_disk(test_save_path)\n",
    "\n",
    "# Load trained model\n",
    "model = ViTForImageClassification.from_pretrained('./model')\n",
    "\n",
    "# Load feature extractor\n",
    "feature_extractor = ViTFeatureExtractor.from_pretrained('./model')\n",
    "    \n",
    "# Create Trainer instance\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=default_data_collator,\n",
    "    tokenizer=feature_extractor\n",
    ")\n",
    "\n",
    "# Evaluate model\n",
    "eval_results = trainer.evaluate(eval_dataset=test_dataset)\n",
    "\n",
    "# Writes eval_result to file which can be accessed later\n",
    "with open(os.path.join(output_data_dir, \"eval_results.json\"), \"w\") as writer:\n",
    "    print(f\"Logging evaluation results at {output_data_dir}/eval_results.json\")\n",
    "    writer.write(json.dumps(eval_results))\n",
    "\n",
    "print(json.dumps(eval_results, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "vit\n",
      "vit.embeddings\n",
      "vit.embeddings.patch_embeddings\n",
      "vit.embeddings.patch_embeddings.projection\n",
      "vit.embeddings.dropout\n",
      "vit.encoder\n",
      "vit.encoder.layer\n",
      "vit.encoder.layer.0\n",
      "vit.encoder.layer.0.attention\n",
      "vit.encoder.layer.0.attention.attention\n",
      "vit.encoder.layer.0.attention.attention.query\n",
      "vit.encoder.layer.0.attention.attention.key\n",
      "vit.encoder.layer.0.attention.attention.value\n",
      "vit.encoder.layer.0.attention.attention.dropout\n",
      "vit.encoder.layer.0.attention.output\n",
      "vit.encoder.layer.0.attention.output.dense\n",
      "vit.encoder.layer.0.attention.output.dropout\n",
      "vit.encoder.layer.0.intermediate\n",
      "vit.encoder.layer.0.intermediate.dense\n",
      "vit.encoder.layer.0.intermediate.intermediate_act_fn\n",
      "vit.encoder.layer.0.output\n",
      "vit.encoder.layer.0.output.dense\n",
      "vit.encoder.layer.0.output.dropout\n",
      "vit.encoder.layer.0.layernorm_before\n",
      "vit.encoder.layer.0.layernorm_after\n",
      "vit.encoder.layer.1\n",
      "vit.encoder.layer.1.attention\n",
      "vit.encoder.layer.1.attention.attention\n",
      "vit.encoder.layer.1.attention.attention.query\n",
      "vit.encoder.layer.1.attention.attention.key\n",
      "vit.encoder.layer.1.attention.attention.value\n",
      "vit.encoder.layer.1.attention.attention.dropout\n",
      "vit.encoder.layer.1.attention.output\n",
      "vit.encoder.layer.1.attention.output.dense\n",
      "vit.encoder.layer.1.attention.output.dropout\n",
      "vit.encoder.layer.1.intermediate\n",
      "vit.encoder.layer.1.intermediate.dense\n",
      "vit.encoder.layer.1.intermediate.intermediate_act_fn\n",
      "vit.encoder.layer.1.output\n",
      "vit.encoder.layer.1.output.dense\n",
      "vit.encoder.layer.1.output.dropout\n",
      "vit.encoder.layer.1.layernorm_before\n",
      "vit.encoder.layer.1.layernorm_after\n",
      "vit.encoder.layer.2\n",
      "vit.encoder.layer.2.attention\n",
      "vit.encoder.layer.2.attention.attention\n",
      "vit.encoder.layer.2.attention.attention.query\n",
      "vit.encoder.layer.2.attention.attention.key\n",
      "vit.encoder.layer.2.attention.attention.value\n",
      "vit.encoder.layer.2.attention.attention.dropout\n",
      "vit.encoder.layer.2.attention.output\n",
      "vit.encoder.layer.2.attention.output.dense\n",
      "vit.encoder.layer.2.attention.output.dropout\n",
      "vit.encoder.layer.2.intermediate\n",
      "vit.encoder.layer.2.intermediate.dense\n",
      "vit.encoder.layer.2.intermediate.intermediate_act_fn\n",
      "vit.encoder.layer.2.output\n",
      "vit.encoder.layer.2.output.dense\n",
      "vit.encoder.layer.2.output.dropout\n",
      "vit.encoder.layer.2.layernorm_before\n",
      "vit.encoder.layer.2.layernorm_after\n",
      "vit.encoder.layer.3\n",
      "vit.encoder.layer.3.attention\n",
      "vit.encoder.layer.3.attention.attention\n",
      "vit.encoder.layer.3.attention.attention.query\n",
      "vit.encoder.layer.3.attention.attention.key\n",
      "vit.encoder.layer.3.attention.attention.value\n",
      "vit.encoder.layer.3.attention.attention.dropout\n",
      "vit.encoder.layer.3.attention.output\n",
      "vit.encoder.layer.3.attention.output.dense\n",
      "vit.encoder.layer.3.attention.output.dropout\n",
      "vit.encoder.layer.3.intermediate\n",
      "vit.encoder.layer.3.intermediate.dense\n",
      "vit.encoder.layer.3.intermediate.intermediate_act_fn\n",
      "vit.encoder.layer.3.output\n",
      "vit.encoder.layer.3.output.dense\n",
      "vit.encoder.layer.3.output.dropout\n",
      "vit.encoder.layer.3.layernorm_before\n",
      "vit.encoder.layer.3.layernorm_after\n",
      "vit.encoder.layer.4\n",
      "vit.encoder.layer.4.attention\n",
      "vit.encoder.layer.4.attention.attention\n",
      "vit.encoder.layer.4.attention.attention.query\n",
      "vit.encoder.layer.4.attention.attention.key\n",
      "vit.encoder.layer.4.attention.attention.value\n",
      "vit.encoder.layer.4.attention.attention.dropout\n",
      "vit.encoder.layer.4.attention.output\n",
      "vit.encoder.layer.4.attention.output.dense\n",
      "vit.encoder.layer.4.attention.output.dropout\n",
      "vit.encoder.layer.4.intermediate\n",
      "vit.encoder.layer.4.intermediate.dense\n",
      "vit.encoder.layer.4.intermediate.intermediate_act_fn\n",
      "vit.encoder.layer.4.output\n",
      "vit.encoder.layer.4.output.dense\n",
      "vit.encoder.layer.4.output.dropout\n",
      "vit.encoder.layer.4.layernorm_before\n",
      "vit.encoder.layer.4.layernorm_after\n",
      "vit.encoder.layer.5\n",
      "vit.encoder.layer.5.attention\n",
      "vit.encoder.layer.5.attention.attention\n",
      "vit.encoder.layer.5.attention.attention.query\n",
      "vit.encoder.layer.5.attention.attention.key\n",
      "vit.encoder.layer.5.attention.attention.value\n",
      "vit.encoder.layer.5.attention.attention.dropout\n",
      "vit.encoder.layer.5.attention.output\n",
      "vit.encoder.layer.5.attention.output.dense\n",
      "vit.encoder.layer.5.attention.output.dropout\n",
      "vit.encoder.layer.5.intermediate\n",
      "vit.encoder.layer.5.intermediate.dense\n",
      "vit.encoder.layer.5.intermediate.intermediate_act_fn\n",
      "vit.encoder.layer.5.output\n",
      "vit.encoder.layer.5.output.dense\n",
      "vit.encoder.layer.5.output.dropout\n",
      "vit.encoder.layer.5.layernorm_before\n",
      "vit.encoder.layer.5.layernorm_after\n",
      "vit.encoder.layer.6\n",
      "vit.encoder.layer.6.attention\n",
      "vit.encoder.layer.6.attention.attention\n",
      "vit.encoder.layer.6.attention.attention.query\n",
      "vit.encoder.layer.6.attention.attention.key\n",
      "vit.encoder.layer.6.attention.attention.value\n",
      "vit.encoder.layer.6.attention.attention.dropout\n",
      "vit.encoder.layer.6.attention.output\n",
      "vit.encoder.layer.6.attention.output.dense\n",
      "vit.encoder.layer.6.attention.output.dropout\n",
      "vit.encoder.layer.6.intermediate\n",
      "vit.encoder.layer.6.intermediate.dense\n",
      "vit.encoder.layer.6.intermediate.intermediate_act_fn\n",
      "vit.encoder.layer.6.output\n",
      "vit.encoder.layer.6.output.dense\n",
      "vit.encoder.layer.6.output.dropout\n",
      "vit.encoder.layer.6.layernorm_before\n",
      "vit.encoder.layer.6.layernorm_after\n",
      "vit.encoder.layer.7\n",
      "vit.encoder.layer.7.attention\n",
      "vit.encoder.layer.7.attention.attention\n",
      "vit.encoder.layer.7.attention.attention.query\n",
      "vit.encoder.layer.7.attention.attention.key\n",
      "vit.encoder.layer.7.attention.attention.value\n",
      "vit.encoder.layer.7.attention.attention.dropout\n",
      "vit.encoder.layer.7.attention.output\n",
      "vit.encoder.layer.7.attention.output.dense\n",
      "vit.encoder.layer.7.attention.output.dropout\n",
      "vit.encoder.layer.7.intermediate\n",
      "vit.encoder.layer.7.intermediate.dense\n",
      "vit.encoder.layer.7.intermediate.intermediate_act_fn\n",
      "vit.encoder.layer.7.output\n",
      "vit.encoder.layer.7.output.dense\n",
      "vit.encoder.layer.7.output.dropout\n",
      "vit.encoder.layer.7.layernorm_before\n",
      "vit.encoder.layer.7.layernorm_after\n",
      "vit.encoder.layer.8\n",
      "vit.encoder.layer.8.attention\n",
      "vit.encoder.layer.8.attention.attention\n",
      "vit.encoder.layer.8.attention.attention.query\n",
      "vit.encoder.layer.8.attention.attention.key\n",
      "vit.encoder.layer.8.attention.attention.value\n",
      "vit.encoder.layer.8.attention.attention.dropout\n",
      "vit.encoder.layer.8.attention.output\n",
      "vit.encoder.layer.8.attention.output.dense\n",
      "vit.encoder.layer.8.attention.output.dropout\n",
      "vit.encoder.layer.8.intermediate\n",
      "vit.encoder.layer.8.intermediate.dense\n",
      "vit.encoder.layer.8.intermediate.intermediate_act_fn\n",
      "vit.encoder.layer.8.output\n",
      "vit.encoder.layer.8.output.dense\n",
      "vit.encoder.layer.8.output.dropout\n",
      "vit.encoder.layer.8.layernorm_before\n",
      "vit.encoder.layer.8.layernorm_after\n",
      "vit.encoder.layer.9\n",
      "vit.encoder.layer.9.attention\n",
      "vit.encoder.layer.9.attention.attention\n",
      "vit.encoder.layer.9.attention.attention.query\n",
      "vit.encoder.layer.9.attention.attention.key\n",
      "vit.encoder.layer.9.attention.attention.value\n",
      "vit.encoder.layer.9.attention.attention.dropout\n",
      "vit.encoder.layer.9.attention.output\n",
      "vit.encoder.layer.9.attention.output.dense\n",
      "vit.encoder.layer.9.attention.output.dropout\n",
      "vit.encoder.layer.9.intermediate\n",
      "vit.encoder.layer.9.intermediate.dense\n",
      "vit.encoder.layer.9.intermediate.intermediate_act_fn\n",
      "vit.encoder.layer.9.output\n",
      "vit.encoder.layer.9.output.dense\n",
      "vit.encoder.layer.9.output.dropout\n",
      "vit.encoder.layer.9.layernorm_before\n",
      "vit.encoder.layer.9.layernorm_after\n",
      "vit.encoder.layer.10\n",
      "vit.encoder.layer.10.attention\n",
      "vit.encoder.layer.10.attention.attention\n",
      "vit.encoder.layer.10.attention.attention.query\n",
      "vit.encoder.layer.10.attention.attention.key\n",
      "vit.encoder.layer.10.attention.attention.value\n",
      "vit.encoder.layer.10.attention.attention.dropout\n",
      "vit.encoder.layer.10.attention.output\n",
      "vit.encoder.layer.10.attention.output.dense\n",
      "vit.encoder.layer.10.attention.output.dropout\n",
      "vit.encoder.layer.10.intermediate\n",
      "vit.encoder.layer.10.intermediate.dense\n",
      "vit.encoder.layer.10.intermediate.intermediate_act_fn\n",
      "vit.encoder.layer.10.output\n",
      "vit.encoder.layer.10.output.dense\n",
      "vit.encoder.layer.10.output.dropout\n",
      "vit.encoder.layer.10.layernorm_before\n",
      "vit.encoder.layer.10.layernorm_after\n",
      "vit.encoder.layer.11\n",
      "vit.encoder.layer.11.attention\n",
      "vit.encoder.layer.11.attention.attention\n",
      "vit.encoder.layer.11.attention.attention.query\n",
      "vit.encoder.layer.11.attention.attention.key\n",
      "vit.encoder.layer.11.attention.attention.value\n",
      "vit.encoder.layer.11.attention.attention.dropout\n",
      "vit.encoder.layer.11.attention.output\n",
      "vit.encoder.layer.11.attention.output.dense\n",
      "vit.encoder.layer.11.attention.output.dropout\n",
      "vit.encoder.layer.11.intermediate\n",
      "vit.encoder.layer.11.intermediate.dense\n",
      "vit.encoder.layer.11.intermediate.intermediate_act_fn\n",
      "vit.encoder.layer.11.output\n",
      "vit.encoder.layer.11.output.dense\n",
      "vit.encoder.layer.11.output.dropout\n",
      "vit.encoder.layer.11.layernorm_before\n",
      "vit.encoder.layer.11.layernorm_after\n",
      "vit.layernorm\n",
      "classifier\n"
     ]
    }
   ],
   "source": [
    "for name, module in model.named_modules():\n",
    "    print(name)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_layer = 'vit.encoder.layer[0]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
